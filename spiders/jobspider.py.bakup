# -*- coding: utf-8 -*-

import re
from jobs import settings
import random
#import scrapy #导入scrapy包
from bs4 import BeautifulSoup
#from scrapy.http import Request , FormRequest##一个单独的request的模块，需要跟进URL的时候，需要用它
from jobs.items import JobsItem ##这是我定义的需要保存的字段，（导入dingdian项目中，items文件中的DingdianItem类）
from jobs.mysqlpipelines.sql import Sql#sql connection
from urllib.parse import urlparse 
from selenium import webdriver
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC 
from selenium.webdriver.remote.webelement import WebElement
from selenium.common.exceptions import StaleElementReferenceException 
import logging
import datetime 

import sys


class JobspiderSpider(scrapy.Spider):


    name = 'jobspider'
    def get_headers(self):
        ua = random.choice(settings.USER_AGENTS)
        headers = {
            "User-Agent": ua #'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1'
            }
        return headers


            
    def start_requests(self):
        


        bash_url = 'https://www.jobs.gov.hk'    
        driver = webdriver.PhantomJS(executable_path='/usr/local/lib/node_modules/phantomjs-prebuilt/bin/phantomjs')
        driver.get(bash_url)
        self.waitForLoad(driver)
        theresponse = driver.page_source
        soup = BeautifulSoup(theresponse,'lxml')
        for a in soup.find_all('a',href=True):
            if a['href'].find("lang=tc") >0:
                lang_url=(a['href'])
        driver.get(lang_url)
        select = Select(driver.find_element_by_id("ctl00_uxSimpLocation"))
        select.select_by_visible_text("--全香港島--")
        driver.find_element_by_id("ctl00_uxSimSearch").click()
#        self.logger.log(logging.INFO, "time" +str(datetime.datetime.now()))
        for i in range(0,20):
             joblist = driver.find_element_by_xpath("//div[@id='uxItemLink_"+ str(i+1)+"']/table/tbody/tr/td[2]")
#             self.logger.log(logging.INFO, BeautifulSoup(joblist.get_attribute('innerHTML'), 'lxml'))
             joblist.click()
             jobselems = joblist.find_elements_by_tag_name("span")
             for j in jobselems:
                 print(BeautifulSoup(j.get_attribute('innerHTML'),'lxml').prettify())
             job =JobsItem()
             
             job ['vacant'] = driver.find_element_by_id("ctl00_ContentPlaceHolder1_uxJobCard_uxNoOfVac").text
             job ['company'] = driver.find_element_by_id("ctl00_ContentPlaceHolder1_uxJobCard_uxCompany").text
             job ['name'] = jobselems[0].text
             job ['job_id'] = driver.find_element_by_id("ctl00_ContentPlaceHolder1_uxJobCard_uxOrdNo").text
             job ['detail'] = driver.find_element_by_xpath("//div[@id='ctl00_ContentPlaceHolder1_uxJobCard_uxJcard']/table[2]/tbody/tr[5]/td").text
             job ['salary'] = jobselems[1].text
             job ['area'] = jobselems[2].text
             job ['date_posted'] = driver.find_element_by_id("ctl00_ContentPlaceHolder1_uxJobCard_uxPostedDate").text
#             self.logger.log(logging.INFO, job)
             yield job


    
    def waitForLoad(self, driver):
        elem = driver.find_element_by_tag_name("html")
        count = 0 
        while True:
            count+=1
            if count >20 :
#                self.logger.log(logging.INFO, "time out")
                return
            time.sleep(.5)
            try:
                elem == driver.find_element_by_tag_name("html")
            except StaleElementReferenceException:
                return


           
    def main_page_url(self, response):
    
        yield FormRequest.from_response(response,
                                formname="aspnetForm",
                                clickdata={'ctl00$uxSimpLocation': '--全香港島--'},
                                callback=self.parse_list)
       
    def parse_list(self,response):
        
        td_job = BeautifulSoup(response.text, 'lxml').find_all('td',class_="job_sort_bg")
      
        for td in td_job:
            
            job_getJobCardDetail_tag = td.select("div")[0]
            js_getJobCardDetail = job_getJobCardDetail_tag.get('onclick')
            
            
            job_js = "javascript:("+ js_getJobCardDetail+")"
#            self.logger.log(logging.INFO, job_js)


            pass
        
        
    
    def movie_page_url(self,response):
#        '''    self.logger.log(logging.INFO, response.url)
        max_num = BeautifulSoup(response.text,'lxml').find('div',class_='paginator').find_all('a')[-2].get_text()
        for i in range(0,int(max_num)):
        page_url = response.url+'?start='+str(i*20)+'&type=T'  #https://movie.douban.com/tag/%E7%88%B1%E6%83%85?start=7840&type=T
    
            yield Request(
                url=page_url,
                headers =self.get_headers(),
                meta = {
                    'category' : response.meta['category'],
                },
                callback = self.get_movie_url)
                '''
        pass
'''
    def get_movie_url(self,response):
        atitle2 = BeautifulSoup(response.text,'lxml').find_all('a',class_='nbg')
        for a in atitle2:
            movie_url = a['href']
            name=a['title']
            movie_id = movie_url.replace('https://movie.douban.com/subject/','').replace('/','')
            yield Request(
                url=movie_url,
                headers =self.get_headers(),
                meta = {
                    'name':name,
                    'movie_id' : movie_id,
                    'category' : response.meta['category'],
                },
                callback = self.get_movie_info)

    def get_movie_info(self,response):#最后的电影页面信息，这里仅仅爬取了少量信息，以后还可以添加
        item = JobsItem()
        score = BeautifulSoup(response.text,'lxml').find('strong',class_='ll rating_num').get_text()
        #ulSoup(response.text,'lxml').find('div',id='info')
        item['name'] = response.meta['name']
        item['score'] = score
        item['movie_id']=response.meta['movie_id']
        item['category'] = response.meta['category']
        return item
        '''